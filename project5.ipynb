{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5 Report\n",
    "\n",
    "## Team members: \n",
    "\n",
    "Jefferson Roylance\n",
    "\n",
    "Justin Fairbourn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we decided to use a dataset of sarcastic and non-sarcastic comments to see if we could classify them correctly using the naive bayes classifier. We combined two ideas - textual analysis of the comment itself, and analysis of the rest of the information about the comment contained in the dataset. In the end, our results were intriguing but we don't feel that it's quite good enough for widespread use. Ideas for further exploration include using k-nearest-neighbors or a neural network in addition to bayes to improve classification ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we used was a set of over a million comments, balanced between sarcastic and non-sarcastic. The data was loaded from the .csv file, packaged into an array, and then processed into separate lists containing the data and the labels. The columns of the original dataset are as follows: \n",
    "\n",
    "* label\n",
    "* comment\n",
    "* author\n",
    "* subreddit\n",
    "* score (# of upvotes - # of downvotes)\n",
    "* ups (# of upvotes)\n",
    "* downs (# of downvotes)\n",
    "* date\n",
    "* created_utc\n",
    "* parent_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis technique\n",
    "\n",
    "For our analysis, we used the multinomial naive bayes classifier, since most of our data was not normally distributed and tests confirmed that using the multinomial classifier was more effective than the gaussian classifier. \n",
    "\n",
    "For the textual analysis of the comment, each comment was converted into a long array containing boolean and number values representing the following characteristics: \n",
    "\n",
    "* Amounts of each letter\n",
    "* Length of the comment (in characters)\n",
    "* Presence of punctuation (boolean)\n",
    "* Average word length\n",
    "* Words used - this was found out by taking the top 500 words used in all comments and then finding the counts of each of those words in the comment\n",
    "* Checking for predefined patterns (we only got around to checking for the presence of '...')\n",
    "* Number of uppercase letters\n",
    "\n",
    "This information was fed into a classifier, which was cross-validated 4 times and scored using the f-score. \n",
    "\n",
    "### Justin analysis\n",
    "\n",
    "In addition to the comments themselves, we wanted to see if we could build a classifier that can detect sarcasm using the subreddit and score data. For memory reasons, we only considered the top 15 subreddits by summing up the frequencies of each subreddit. This gave us a 1,000,000 x 22 DataFrame with \"dummy\" variables that allowed us to represent the categorical subreddit data numerically.\n",
    "\n",
    "Then, we created a Naive-Bayes classifier and fed it the following attributes:\n",
    " * The total amount of \"upvotes\" the comment received.\n",
    " * The total amount of \"downvotes\" the comment received.\n",
    " * 15 dummy boolean variables representing the comment's subreddit.\n",
    " \n",
    "Not satisfied with the f-scores, we also ran another analysis and instead fed the classifier _only_ the subreddit data. Surprisingly, this gave slightly better performance.\n",
    "\n",
    "In both of these analyses, cross-validation was performed 20 times.\n",
    "\n",
    "### Combined analysis\n",
    "\n",
    "Wanting to get the best results possible, we decided to combine the feature sets used in the previous two analyses to create one giant featureset with both the word features and the subreddit features.\n",
    "\n",
    "Unfortunately, this led to some intense memory issues, so we needed to reduce the amount of comments we were analyzing to 10000 in order to successfully fit our 500 features into the Naive-Bayes classifier. Once this was done succesfully, we ran cross-validation 5 times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Average f-score with only comment textual analysis (using all comments):  0.6209258413787513\n",
    "\n",
    "Average f-score with subreddit and score data (using comments in the top 15 subreddits): 0.4480299005407648\n",
    "\n",
    "Average f-score with just subreddit data (no scores): 0.5123293945443769\n",
    "\n",
    "Average f-score with subreddit data and comment textual analysis: pending due to MemoryError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Explained\n",
    "\n",
    "The f-scores above indicate that the filter was most effective when only considering the comment's text. By finding the most common words used in the top 10,000 comments we were able to build a sarcasm filter that could successfully detect sarcasm more than 50% of the time. \n",
    "\n",
    "By comparison, a filter that just relied on the subreddit and score data detected sarcasm less than 50% of the time, while a filter on subreddit data was barely above 50%. \n",
    "\n",
    "__Overall__: The results hint that _where_ a sarcastic Reddit comment is posted is not as important as _what_ the comment actually contains.\n",
    "\n",
    "### Improvements\n",
    "\n",
    "To improve the performance of this filter, the first thing we would need to do is optimize the way it handles the memory of the analysis. This will allow us to analyze more comments and subreddits more quickly, and allow us to handle more meaningful feature sets to provide better f-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project X Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments relating to code snippet 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"data/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = [(\n",
    "    row['label'],\n",
    "    str(row['comment']),\n",
    ") for index, row in comments[:10000].iterrows()]\n",
    "# random.shuffle(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13679"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigString = ' '.join([comment[1] for comment in comment_list])\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  bigString.lower()).split()\n",
    "\n",
    "bigDict = {}\n",
    "for word in wordList:\n",
    "    if word in bigDict:\n",
    "        bigDict[word] += 1\n",
    "    else:\n",
    "        bigDict[word] = 1\n",
    "display(len(bigDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'a', 'i', 'to', 'you', 'it', 'and', 'that', 'is', 'of']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topWords = sorted(bigDict, key=bigDict.__getitem__, reverse=True)[:500]\n",
    "display(topWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz1234567890'\n",
    "uppercaseAlphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "punctuation = '*\\\"\\(\\)~:/&\\'.-#[];_^$\\{\\}=!+?@%`,|\\x08'\n",
    "patterns = ['...']\n",
    "alphabet += punctuation\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([l for l in alphabet])\n",
    "\n",
    "def comment_features_word(comment):\n",
    "    features = []\n",
    "\n",
    "    # Letter counts\n",
    "    for letter in alphabet:\n",
    "        features.append(comment.lower().count(letter))\n",
    "        \n",
    "    # Length\n",
    "    features.append(len(comment))\n",
    "    \n",
    "    # Presence of punctuation\n",
    "    punctPresence = False\n",
    "    for p in punctuation:\n",
    "        punctPresence = punctPresence or p in comment\n",
    "    features.append(punctPresence)\n",
    "    \n",
    "    # Average word length\n",
    "    commentWords = re.sub(\"[^\\w]\", \" \",  comment).split()\n",
    "    a = sum([len(word) for word in commentWords]) / len(commentWords) if len(commentWords) > 0 else 0\n",
    "    features.append(a)\n",
    "    \n",
    "    # Words used\n",
    "    a = []\n",
    "    for word in topWords:\n",
    "        a.append(comment.lower().count(word))\n",
    "    features.extend(a)\n",
    "    \n",
    "    # Checking for predefined patterns\n",
    "    a = []\n",
    "    for pattern in patterns:\n",
    "        a.append(comment.count(pattern))\n",
    "    features.extend(a)\n",
    "    \n",
    "    # Checking for number of uppercase letters\n",
    "    a = 0\n",
    "    for letter in uppercaseAlphabet:\n",
    "        a += comment.count(letter)\n",
    "    features.append(a)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [comment_features_word(comment) for (_, comment) in comment_list]\n",
    "y = [label for (label, _) in comment_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "cv_results = cross_validate(clf, X, y, cv=4, scoring='f1')\n",
    "\n",
    "print(\"Average f-score: \", sum(cv_results['test_score']) / len(cv_results['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subreddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    183069\n",
       "0    169832\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subreddit_counts = comments['subreddit'].value_counts()\n",
    "comments_counts = comments.join(subreddit_counts, on='subreddit', rsuffix='_count')\n",
    "comments_counts = comments_counts[comments_counts.subreddit_count > 8000]\n",
    "subreddit_data = pd.get_dummies(comments_counts['subreddit'], prefix='r',sparse=True)\n",
    "display(comments_counts['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4480299005407648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6316247059037982"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop everything except for the subreddit dummy values and the up/down scores and the labels\n",
    "comments_test = comments_counts.drop(['comment', 'subreddit', 'author', 'date', 'created_utc', 'parent_comment','score','subreddit_count'], axis=1)\n",
    "dummy_comments = pd.concat([comments_test, subreddit_data], axis=1)\n",
    "\n",
    "# Take absolute value of each score since NBC's don't like negative numbers\n",
    "dummy_comments['ups'] = abs(dummy_comments['ups'])\n",
    "dummy_comments['downs'] = abs(dummy_comments['downs'])\n",
    "\n",
    "X = dummy_comments.drop('label', axis=1).values\n",
    "y = dummy_comments['label'].values\n",
    "\n",
    "f_scores_0 = []\n",
    "f_scores_1 = []\n",
    "\n",
    "for _ in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    f_scores_0.append(f[0])\n",
    "    f_scores_1.append(f[1])\n",
    "\n",
    "display(sum(f_scores_0) / float(len(f_scores_0)))\n",
    "display(sum(f_scores_1) / float(len(f_scores_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5123293945443769"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6119059818643958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop everything except for the subreddit dummy values and the labels\n",
    "comments_test_no_scores = comments_counts.drop(['comment', 'subreddit', 'author', 'date', 'created_utc', 'parent_comment','score','subreddit_count','ups','downs'], axis=1)\n",
    "dummy_comments_no_scores = pd.concat([comments_test_no_scores, subreddit_data], axis=1)\n",
    "\n",
    "X = dummy_comments_no_scores.drop('label', axis=1).values\n",
    "y = dummy_comments_no_scores['label'].values\n",
    "\n",
    "f_scores_0_no_scores = []\n",
    "f_scores_1_no_scores = []\n",
    "\n",
    "for _ in range(20):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    f_scores_0_no_scores.append(f[0])\n",
    "    f_scores_1_no_scores.append(f[1])\n",
    "    \n",
    "display(sum(f_scores_0_no_scores) / float(len(f_scores_0_no_scores)))\n",
    "display(sum(f_scores_1_no_scores) / float(len(f_scores_1_no_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The f-scores for the data _without_ the scores was slightly better, we'll combine those features with the comment word features to do our final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-029a295abfeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcomments_test_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcomments_test_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_test_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'author'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'created_utc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'parent_comment'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'subreddit_count'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ups'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'downs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcomments_test_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomments_test_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubreddit_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_test_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'comment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_new_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m_get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m                 \u001b[0mnew_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_comb_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_axes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m_get_comb_axis\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    467\u001b[0m             return _get_objs_combined_axis(self.objs, axis=data_axis,\n\u001b[0;32m    468\u001b[0m                                            \u001b[0mintersect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m                                            sort=self.sort)\n\u001b[0m\u001b[0;32m    470\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\api.py\u001b[0m in \u001b[0;36m_get_objs_combined_axis\u001b[1;34m(objs, intersect, axis, sort)\u001b[0m\n\u001b[0;32m     68\u001b[0m                  if hasattr(obj, '_get_axis')]\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobs_idxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_get_combined_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_idxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintersect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintersect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\api.py\u001b[0m in \u001b[0;36m_get_combined_index\u001b[1;34m(indexes, intersect, sort)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_union_indexes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\api.py\u001b[0m in \u001b[0;36m_union_indexes\u001b[1;34m(indexes, sort)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'array'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36munion\u001b[1;34m(self, other, sort)\u001b[0m\n\u001b[0;32m   2320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_monotonic\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_monotonic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2322\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_outer_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m                 \u001b[1;31m# incomparable objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\justi\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_outer_indexer\u001b[1;34m(self, left, right)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_outer_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlibjoin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_join_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[0m_typ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\join.pyx\u001b[0m in \u001b[0;36mpandas._libs.join.outer_join_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Drop everything except for the subreddit dummy values and the labels\n",
    "comments_test_full = comments_counts.head(1000)\n",
    "comments_test_full = comments_test_full.drop(['subreddit', 'author', 'date', 'created_utc', 'parent_comment','score','subreddit_count','ups','downs'], axis=1)\n",
    "comments_test_full = pd.concat([comments_test_full, subreddit_data], axis=1)\n",
    "\n",
    "X = comments_test_full.drop(['label','comment'], axis=1).values.tolist()\n",
    "y = comments_test_full['label'].values.tolist()\n",
    "comments = comments_test_full['comment'].values.tolist()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    next_comment = str(comments[i])\n",
    "    X[i].extend(comment_features_word(next_comment))\n",
    "\n",
    "\n",
    "comments_test_full = []\n",
    "\n",
    "print('finished extending')\n",
    "f_scores_0_no_scores = []\n",
    "f_scores_1_no_scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    p,r,f,s = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    f_scores_0_no_scores.append(f[0])\n",
    "    f_scores_1_no_scores.append(f[1])\n",
    "    \n",
    "display(sum(f_scores_0_no_scores) / float(len(f_scores_0_no_scores)))\n",
    "display(sum(f_scores_1_no_scores) / float(len(f_scores_1_no_scores)))\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "comments_test_full = None\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "clf = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
